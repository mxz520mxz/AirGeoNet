experiment:
  name: ???
  gpus: 3
  seed: 0
training:
  lr: 1e-4
  lr_scheduler: null
  finetune_from_checkpoint: null
  trainer:
    val_check_interval: 50
    log_every_n_steps: 10
    limit_val_batches: 50
    max_steps: 200000
    devices: ${experiment.gpus}
  checkpointing:
    monitor: "loss/total/val"
    save_top_k: 5
    mode: "min"
hydra:
  job:
    name: ${experiment.name}
    chdir: false
